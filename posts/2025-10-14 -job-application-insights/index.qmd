---
title: "Minha jornada de 3 meses e 400 candidaturas: insights de dados para recolocação"
author: "Érika Borelli"
date: "2025-10-27"
categories: [code, analysis, job]
image: "image.jpg"
---

Analisei meu histórico de candidaturas e identifiquei padrões sobre o tempo de resposta, as ferramentas mais exigidas e o tipo de vaga que gerou maior retorno das empresas. Neste post apresento os principais insights obtidos nessa jornada, com visualizações e reflexões práticas para quem também está buscando recolocação na área de dados.

## Como os dados foram obtidos

| Etapa | Descrição |
|------------------------------------|------------------------------------|
| **Fonte dos dados** | Cartões de vagas no meu Trello + logs de atividades |
| **Pré-processamento** | Python e R, com uso de `dplyr`, `tibble`, `lubridate` e `tidytext` |
| **Número de observações** | Total de candidaturas avaliadas |

## Principais aspectos analisados

-   Volume total de candidaturas

-   Tempo até resposta das empresas\
    Distribuição, média e mediana por tipo de retorno

-   Idioma predominante das vagas

-   Palavras-chave e ferramentas técnicas mais mencionadas

-   Taxa de resposta por tipo de posição

## Duração do processo de candidaturas

```{r}
#| echo: false
#| message: false
#| include: false
box::use(
    readr[read_csv],
  dplyr[arrange, group_by, filter, mutate,
        summarise, n, left_join, select,
        rename, join_by, ungroup, across,
        if_else, first, case_when, count,
         desc, distinct, slice_max,
        bind_rows, pull, row_number,
        n_distinct, slice_head],
  lubridate[dmy, interval, time_length, ymd_hms],
  tidyr[replace_na],
  plotly[plot_ly, layout, add_bars, add_trace, subplot ],
  tibble[tibble],
  stringr[str_to_lower]
)


cards <- read_csv("~/Analises/Trello_analytics/cards_clean.csv")
actions <-  read_csv("~/Analises/Trello_analytics/actions_trello.csv")
comments <- read_csv("~/Analises/Trello_analytics/comments_trello.csv")
requisitos <- read_csv("~/Analises/Trello_analytics/requisitos_limpos_final.csv") |> filter(habilidade != 'analise de dados')



```

```{r}
#| echo: false
#| message: false
#| include: false
# Duração do processo
data_inicio <- min(actions$date, na.rm = TRUE)
data_fim <- dmy("29/08/25")
meses_decimais <- round(time_length(interval(data_inicio, data_fim), "months"),2)

# Total candidaturas
total_candidatura <- cards |>
  summarise( total = n())


```

```{r}
#| echo: false
#| message: false
#| include: false
# Candidaturas com resposta
date_first_action <- actions |>
  filter(data.list.name %in% c("Com resposta", "Finalizado")) |>
  group_by(data.card.id) |>
  filter(date == min(date)) |>
  select(data.list.name, data.card.id, date) |>
  rename(date_first_reply = date, list_name_reply = data.list.name)
  
first_date_card <-  actions |>
  group_by(data.card.id) |>
  filter(date == min(date)) |>
  select(data.list.name, data.card.id, date) |>
  rename(date_first_action= date, list_name_first =data.list.name )

cards_with_answer <- first_date_card |>
  left_join(date_first_action)

answer_duration <- cards_with_answer |>
  filter(!is.na(list_name_reply)) |>
  mutate(
    duration = time_length(interval(ymd_hms( date_first_action), ymd_hms(date_first_reply)), "days")
  ) 

empresas <- cards |> 
  group_by(empresa) |> 
  summarise( total_vagas = n())

unique_empresas  = empresas |> distinct(empresa) |> count()

# Colocar isso logo após suas primeiras transformações, antes da seção "Distribuição das Candidaturas"

date_first_reply <- actions |>
  filter(data.list.name %in% c("Com resposta", "Finalizado")) |>
  group_by(data.card.id) |>
  summarise(
    date_first_reply = min(date),
    .groups = "drop"
  )

# CRIAR ISSO ANTES da seção "Distribuição das Candidaturas"

date_first_reply_completo <- actions |>
  filter(data.list.name %in% c("Com resposta", "Finalizado")) |>
  group_by(data.card.id) |>
  summarise(
    list_name_reply = first(data.list.name),  # ← ESTA COLUNA FALTAVA
    date_first_reply = min(date),
    .groups = "drop"
  )

first_date_card <- actions |>
  group_by(data.card.id) |>
  summarise(
    date_first_action = min(date),
    .groups = "drop"
  )

# BASE PARA STATUS (usada em fonte_status)
cards_base_status <- first_date_card |>
  left_join(date_first_reply_completo, by = "data.card.id") |>
  mutate(
    date_first_action = ymd_hms(date_first_action),
    date_first_reply = ymd_hms(date_first_reply),
    status_final = case_when(
      !is.na(list_name_reply) & list_name_reply == "Com resposta" ~ "Com resposta",
      !is.na(list_name_reply) & list_name_reply == "Finalizado" ~ "Finalizado",
      TRUE ~ "Sem resposta"
    )
  )

# BASE PARA OVERDUE (usada em análises de tempo)
cards_base_overdue <- first_date_card |>
  left_join(date_first_reply_completo, by = "data.card.id") |>
  mutate(
    date_first_action = ymd_hms(date_first_action),
    date_first_reply  = ymd_hms(date_first_reply),
    reply_bucket = if_else(!is.na(date_first_reply), "Com resposta", "Sem resposta"),
    data_limite = if_else(
      reply_bucket == "Sem resposta",
      data_fim,
      date_first_reply
    ),
    duration_days = time_length(interval(date_first_action, data_limite), "days"),
    overdue_15 = duration_days > 15
  )

```

Monitorei minhas candidaturas ao longo de aproximadamente `r meses_decimais` meses. Durante esse período, registrei **`r total_candidatura$total` candidaturas** direcionadas a **`r unique_empresas` empresas distintas**, acompanhando todas as movimentações até o primeiro retorno, seja ele positivo ou negativo.

Essa janela de acompanhamento permitiu observar tanto a dinâmica das empresas em cada fase do processo seletivo quanto os momentos em que o mercado se mostrou mais responsivo a novas aplicações.

*Dado de referência: duração total desde a primeira candidatura registrada até 29/08/2025 (data da minha contratação).*

## Distribuição das Candidaturas: Idioma, Canais e Tipos de Vaga

```{r}
#| echo: false
#| message: false

lang_summ <- cards |>
  group_by(desc_lang) |>
  summarise(count_id = n())

fonte_summ <- cards |>
  group_by(fonte_standard) |>
  summarise(count_id = n()) |>
  arrange(desc(count_id))

nomes_vagas_summ <- cards |>
  group_by(vaga_standard) |>
  summarise(count_id = n()) |>
  arrange(desc(count_id))

fonte_status <- cards |>
  select(id, fonte_standard) |>
  left_join(cards_base_status |>  # ← AGORA USA A BASE CORRETA
              select(data.card.id, status_final) |>
              rename(id = data.card.id)) |>
  group_by(fonte_standard, status_final) |>
  summarise(count_id = n()) |>
  arrange(desc(count_id))

# Cálculos para o texto
total_candidaturas <- sum(lang_summ$count_id)

# Estatísticas de idioma
portugues_pct <- lang_summ |> filter(desc_lang == "Portuguese") |> pull(count_id) / total_candidaturas * 100
ingles_pct <- lang_summ |> filter(desc_lang == "english") |> pull(count_id) / total_candidaturas * 100

# Estatísticas de fontes
top_fonte <- fonte_summ |> slice_max(count_id, n = 1)
top_fonte_nome <- top_fonte$fonte_standard
top_fonte_count <- top_fonte$count_id
top_fonte_pct <- round((top_fonte_count / total_candidaturas) * 100, 1)

# Estatísticas de vagas
top_vaga <- nomes_vagas_summ |> slice_max(count_id, n = 1)
top_vaga_nome <- top_vaga$vaga_standard
top_vaga_count <- top_vaga$count_id
top_vaga_pct <- round((top_vaga_count / total_candidaturas) * 100, 1)

# Taxa de resposta por fonte
taxa_linkedin <- fonte_status |> 
  filter(fonte_standard == "linkedin", status_final == "Com resposta") |> 
  pull(count_id)
total_linkedin <- fonte_summ |> filter(fonte_standard == "linkedin") |> pull(count_id)
taxa_linkedin_pct <- round((taxa_linkedin / total_linkedin) * 100, 1)
```

### Concentração Geográfica e Estratégia de Idioma

```{r}
#| echo: false
#| message: false
# Gráfico de pizza - Idioma das vagas
fig <- plot_ly(
  data = lang_summ,
  labels = ~desc_lang,
  values = ~count_id,
  type = "pie",
  hole = 0.6,
  textinfo = "label+percent",
  textfont = list(size = 14, color = "#111827"),
  marker = list(colors = c("#3B82F6", "#F59E0B", "#9CA3AF")),
  showlegend = FALSE
) |>
  layout(
    title = list(
      text = "Idioma das Descrições das Vagas",
      font = list(family = "Arial", size = 18, color = "#111827")
    ),
    plot_bgcolor = "#FFFFFF",
    paper_bgcolor = "#FFFFFF"
  )

fig
```

A distribuição por idioma revela uma estratégia focada principalmente no mercado brasileiro, com `r round(portugues_pct, 1)`% das vagas em português. No entanto, busquei diversificar com oportunidades internacionais, representando `r round(ingles_pct, 1)`% das candidaturas em inglês, uma tentativa de aproveitar minha experiência internacional e ampliar as possibilidades de atuação.

### Dominância do LinkedIn como Canal Principal

```{r}
#| echo: false
#| message: false
# Gráfico de barras - Top 5 fontes
fig_fonte <- fonte_summ |>
  arrange(desc(count_id)) |>
  filter(fonte_standard != 'none') |>
  head(5) |>
  plot_ly(
    x = ~count_id,
    y = ~reorder(fonte_standard, count_id),
    text = ~count_id,
    type = 'bar',
    orientation = 'h',
    textposition = 'auto',
    marker = list(color = '#3B82F6')
  ) |>
  layout(
    title = list(
      text = "Principais Canais de Candidatura - Top 5",
      font = list(family = "Arial", size = 18, color = "#111827")
    ),
    xaxis = list(title = "Número de Candidaturas"),
    yaxis = list(title = ""),
    plot_bgcolor = "#FFFFFF",
    paper_bgcolor = "#FFFFFF"
  )

fig_fonte
```

O LinkedIn emergiu como canal dominante, representando `r top_fonte_count` candidaturas (`r top_fonte_pct`% do total). Esta concentração reflete a eficácia da plataforma para busca ativa e a frequência de atualizações diárias que facilitavam a descoberta de oportunidades alinhadas.

Destaque positivo: O LinkedIn também mostrou a melhor taxa de conversão entre os principais canais, com `r taxa_linkedin_pct`% das candidaturas evoluindo para a fase de entrevistas.

### Foco Estratégico em Análise de Dados

```{r}
#| echo: false
#| message: false
# Gráfico de barras - Top 5 vagas
fig_vagas <- nomes_vagas_summ |>
  arrange(desc(count_id)) |>
  head(6) |>  # Incluindo o 6º para mostrar o product analyst
  plot_ly(
    x = ~count_id,
    y = ~reorder(vaga_standard, count_id),
    text = ~count_id,
    type = 'bar',
    orientation = 'h',
    textposition = 'auto',
    marker = list(color = '#10B981')
  ) |>
  layout(
    title = list(
      text = "Tipos de Vaga Mais Frequentes - Top 6",
      font = list(family = "Arial", size = 18, color = "#111827")
    ),
    xaxis = list(title = "Número de Candidaturas"),
    yaxis = list(title = ""),
    plot_bgcolor = "#FFFFFF",
    paper_bgcolor = "#FFFFFF"
  )

fig_vagas
  
  
```

A análise dos cargos mostra uma evolução consciente na estratégia de aplicação. Com o tempo, passei a focar mais em vagas de `r top_vaga_nome`, que representaram `r top_vaga_count` candidaturas (`r top_vaga_pct`%).

Esta mudança reflete um alinhamento mais estratégico entre as oportunidades do mercado e minhas experiências práticas anteriores, priorizando posições onde meu fit técnico e de perfil era mais evidente.

**Distribuição do portfólio de vagas:**

  - `r top_vaga_nome` (`r top_vaga_pct`%): Foco principal baseado em experiência

  - Cientista de Dados (`r round(115/total_candidaturas*100, 1)`%): Busca por oportunidades de crescimento

  - Analista de BI (`r round(50/total_candidaturas*100, 1)`%): Vagas especializadas em visualização

### Insight Consolidado

Esta tripla análise revela uma estratégia coerente: foco no mercado brasileiro via LinkedIn, com evolução para posições de Analista de Dados que melhor se alinhavam com meu perfil técnico e experiências anteriores.

A combinação 'mercado local + canal dominante + fit de perfil' mostrou-se mais eficaz que abordagens genéricas ou pulverizadas.

## Empresas que me candidatei

```{r}
#| echo: false
#| message: false
empresas_more_than_1 <- empresas |>
  filter(total_vagas > 1)
empresas_more_than_1_count <- empresas_more_than_1 |> count()
total_candidaturas_empresas_more_1 <- empresas_more_than_1$total_vagas |> sum()

```

Nesse período de busca por uma nova oportunidade, eu não me limitei a um contato único com cada organização. Do total de `r unique_empresas` empresas distintas, `r empresas_more_than_1_count` receberam mais de uma candidatura da minha parte. Sendo que elas foram alvo de `r total_candidaturas_empresas_more_1` candidaturas.

```{r}
#| echo: false
#| message: false
empresas_top_3 <- empresas |>
  arrange(total_vagas |> desc()) |>
  head(3) |>
  left_join(cards |>
  select(id, vaga_standard, empresa)) |>
  left_join(
    cards_base_overdue  |>
      select(data.card.id, reply_bucket, duration_days, overdue_15) |>
      rename(id = data.card.id)
  )
  
top_1 <-  empresas_more_than_1 |>
  arrange(desc(total_vagas)) |>
  head(1) |>
  left_join(empresas_top_3)

top_1_duration <- top_1 |>
  group_by(reply_bucket) |> 
  summarise(duration_avg = mean(duration_days) |> round(2) ,
            count_reply = n())


```

A `r unique(top_1$empresa)` lidera em número de candidaturas (`r unique(top_1$total_vagas)`), incluindo vagas para "Analista de Dados" e "Analista de BI". Das aplicações enviadas, recebi feedback negativo em `r top_1_duration |> filter(reply_bucket == 'Com resposta') |> pull(count_reply)` casos, com uma média de `r top_1_duration |> filter(reply_bucket == 'Com resposta') |> pull(duration_avg)` dias para resposta. As demais aplicações não obtiveram retorno.

## O Ritmo dos Processos Seletivos

```{r}
#| echo: false
#| message: false

# --- Agregados: total por bucket e quantos "fora do prazo" ---
agg_overdue <- cards_base_overdue  |>
  group_by(reply_bucket) |>
  summarise(
    total_cards   = n(),
    overdue_15    = sum(overdue_15, na.rm = TRUE),
    pct_overdue   = overdue_15 / total_cards,
    .groups = "drop"
  ) |>
  arrange(factor(reply_bucket, levels = c("Com resposta", "Sem resposta")))


# Cálculos para o texto
com_resposta <- agg_overdue |> filter(reply_bucket == "Com resposta")
pct_com_resposta_fora_prazo <- round(com_resposta$pct_overdue * 100, 1)
total_com_resposta <- com_resposta$total_cards
num_fora_prazo <- com_resposta$overdue_15

sem_resposta <- agg_overdue |> filter(reply_bucket == "Sem resposta")
pct_sem_resposta_fora_prazo <- round(sem_resposta$pct_overdue * 100, 1)
```

A análise do tempo de resposta revela dinâmicas importantes sobre o pacing dos processos seletivos:

Das `r total_com_resposta` candidaturas que obtiveram resposta, `r num_fora_prazo` (`r pct_com_resposta_fora_prazo`%) levaram mais de 15 dias para o primeiro retorno

Entre as `r sem_resposta$total_cards` aplicações sem resposta, `r pct_sem_resposta_fora_prazo`% já ultrapassaram a marca de duas semanas


## Taxa de Retorno das Candidaturas

```{r}
#| echo: false
#| message: false

# juntar as informações principais
date_first_reply <- actions |>
  filter(data.list.name %in% c("Com resposta", "Finalizado")) |>
  group_by(data.card.id) |>
  summarise(
    list_name_reply = first(data.list.name),
    date_first_reply = min(date),
    .groups = "drop"
  )

first_date_card <- actions |>
  group_by(data.card.id) |>
  summarise(date_first_action = min(date), .groups = "drop")

cards_base <- first_date_card |>
  left_join(date_first_reply, by = "data.card.id") |>
  mutate(
    date_first_action = ymd_hms(date_first_action),
    date_first_reply = ymd_hms(date_first_reply),
    status_final = case_when(
      !is.na(list_name_reply) & list_name_reply == "Com resposta" ~ "Com resposta",
      !is.na(list_name_reply) & list_name_reply == "Finalizado" ~ "Finalizado",
      TRUE ~ "Sem resposta"
    )
  )

agg_status <- cards_base |>
  count(status_final) |>
  mutate(pct = n / sum(n))

# gráfico de rosca com 3 categorias
fig <- plot_ly(
  data = agg_status,
  labels = ~status_final,
  values = ~pct,
  type = "pie",
  hole = 0.6,
  textinfo = "label+percent",
  textfont = list(size = 14, color = "#111827"),
  marker = list(colors = c("#3B82F6", "#F59E0B", "#9CA3AF")),  # azul = resposta positiva, laranja = finalizado, cinza = sem resposta
  showlegend = FALSE
) |>
  layout(
    title = list(
      text = "Status of Job Applications",
      font = list(family = "Arial", size = 18, color = "#111827")
    ),
    plot_bgcolor = "#FFFFFF",
    paper_bgcolor = "#FFFFFF"
  )

total_candidaturas <- sum(agg_status$n)
finalizado_count <- agg_status |> filter(status_final == "Finalizado") |> pull(n)
finalizado_pct <- agg_status |> filter(status_final == "Finalizado") |> pull(pct)
com_resposta_count <- agg_status |> filter(status_final == "Com resposta") |> pull(n)
com_resposta_pct <- agg_status |> filter(status_final == "Com resposta") |> pull(pct)
sem_resposta_pct <- agg_status |> filter(status_final == "Sem resposta") |> pull(pct)

fig



```

Para categorizar o destino de cada aplicação, classifiquei as candidaturas em três estágios:

- Com resposta: Retorno positivo com continuidade do processo;

-   Finalizado: Resposta encerrando a candidatura;

-   Sem resposta: Nenhum feedback recebido.

A primeira impressão ao ver meus dados foi positiva ,aproximadamente `r round(finalizado_pct * 100, 1)`% das candidaturas tiveram algum tipo de resposta.

Porém, ao separar as respostas que realmente evoluíram para entrevistas, a realidade mostrou-se mais dura: apenas `r round(com_resposta_pct * 100, 1)`% (`r com_resposta_count` vagas) avançaram no processo.

Os outros `r round(finalizado_pct * 100, 1)`% representavam respostas negativas, enquanto `r round(sem_resposta_pct * 100, 1)`% das aplicações permaneceram no silêncio absoluto.

Essa distribuição reflete uma dinâmica comum no mercado de tecnologia: o "funil de candidaturas" é bastante estreito. Para cada vaga que avança para a fase de entrevistas, diversas outras encontram respostas negativas ou simplesmente não obtêm retorno.

O dado me ensinou a importância de gerenciar expectativas e entender que o silêncio não é pessoal, faz parte do processo de busca por oportunidades alinhadas.

## Faster to Hire, Slower to Reject?

```{r}
#| echo: false
#| message: false

dias_resposta_positiva <- answer_duration |> 
  filter(list_name_reply == "Com resposta") |> 
  pull(duration) |> mean() |> round(1)

dias_resposta_negativa <- answer_duration |> 
  filter(list_name_reply == "Finalizado") |> 
  pull(duration) |> mean() |> round(1)

diferenca_dias <- dias_resposta_negativa - dias_resposta_positiva

fig <- answer_duration |>
  group_by(list_name_reply) |>
  summarise(mean_duration = mean(duration) |> round(2)) |>
  plot_ly(
    x = ~mean_duration, 
    y = ~list_name_reply,
    type = 'bar', 
    orientation = 'h',
    textposition = 'auto', 
    text = ~mean_duration,
    marker = list(color = c("#3B82F6", "#9CA3AF")),
    textfont = list(color = "#FFFFFF", size = 14)
  )

fig |>
  layout(
    title = list(
      text = "Tempo Médio para Resposta das Empresas",
      font = list(family = "Arial", size = 18, color = "#111827")
    ),
    yaxis = list(
      title = list(text = "Tipo de Resposta", font = list(size = 14)),
      categoryorder = "total ascending"
    ),
    xaxis = list(
      title = list(text = "Dias em Média", font = list(size = 14)),
      range = c(0, 18)
    ),
    plot_bgcolor = "#FFFFFF",
    paper_bgcolor = "#FFFFFF",
    font = list(family = "Arial", size = 12, color = "#374151")
  )

```

Nos meus dados: respostas positivas chegam em \~`r dias_resposta_positiva` dias, enquanto rejeições formais levam \~`r dias_resposta_negativa` dias em média.

A análise do tempo de resposta revela um padrão contraintuitivo: processos que evoluem para entrevistas tendem a ter retornos mais ágeis.

Enquanto as respostas positivas ("Com resposta") chegaram em média em `r dias_resposta_positiva` dias, as rejeições formais ("Finalizado") levaram `r dias_resposta_negativa` dias, cerca de `r diferenca_dias` dias a mais.

Por que essa diferença? Este gap de r diferenca_dias dias reflete a dinâmica interna dos processos seletivos:

-   `r dias_resposta_positiva` dias para avançar: Sinaliza agilidade quando há fit imediato - as empresas priorizam contatar bons candidatos antes que sejam captados pela concorrência

-   `r dias_resposta_negativa` dias para rejeitar: Pode indicar processos com múltiplas etapas, onde a empresa aguarda conclusão de todas as entrevistas antes de enviar respostas padronizadas



```{r}
#| echo: false
#| message: false
# Cálculos para o texto
outlier_max <- max(answer_duration$duration) |> round(1)
outlier_count <- answer_duration |> filter(duration > 30) |> nrow()
total_respostas <- nrow(answer_duration)
pct_outliers <- round((outlier_count / total_respostas) * 100, 1)

# Estatísticas por grupo
stats_resposta <- answer_duration |>
  group_by(list_name_reply) |>
  summarise(
    mediana = median(duration),
    q1 = quantile(duration, 0.25),
    q3 = quantile(duration, 0.75)
  )
```

```{r}
#| echo: false
#| message: false
fig <- answer_duration |>
  plot_ly(
    y = ~duration, 
    color = ~list_name_reply,
    type = "box",
    boxpoints = "outliers",
    marker = list(color = "#6B7280"),
    line = list(color = "#374151")
  ) |>
  layout(
    title = list(
      text = "Distribuição Completa dos Tempos de Resposta",
      font = list(family = "Arial", size = 18, color = "#111827")
    ),
    yaxis = list(
      title = "Dias até o Primeiro Retorno",
      rangemode = "tozero"
    ),
    xaxis = list(title = ""),
    plot_bgcolor = "#FFFFFF",
    paper_bgcolor = "#FFFFFF",
    font = list(family = "Arial", size = 12)
  )

fig

```

Enquanto as médias nos dão uma visão geral, a distribuição completa revela nuances importantes sobre os processos:

-   `r outlier_count` das `r total_respostas` respostas (`r pct_outliers`%) levaram mais de 30 dias

-   O caso mais extremo registrou `r outlier_max` dias entre candidatura e resposta

-   A mediana para respostas positivas foi de `r stats_resposta |> filter(list_name_reply == "Com resposta") |> pull(mediana) |> round(1)` dias, mostrando que a maioria dos retornos ágeis realmente ocorre nas primeiras semanas

**O Ruído dos Dados Manuais**

A análise revelou outliers curiosos — como uma resposta registrada após r outlier_max dias. Ao investigar, percebi que muitos desses picos vinham do meu próprio processo de registro no Trello, não necessariamente do tempo real de resposta das empresas.

Esses desvios mostram como o gerenciamento manual pode introduzir ruído nos dados — um lembrete importante para qualquer análise baseada em registros pessoais.

**Lições para Coleta de Dados Pessoais**

1.  Timing de registro: O momento de mover um card no Trello nem sempre coincide com o recebimento real do email

2.  Consistência metodológica: Manter padrões rigorosos de registro é crucial para dados confiáveis

3.  Contexto matters: Dados extremos sempre merecem uma verificação qualitativa

Mantive o boxplot para mostrar a dispersão real. Os outliers refletem mais ruído de registro do que o comportamento das empresas — algo comum em dados coletados manualmente que todo analista deve considerar.

## A Evolução da Estratégia de Candidaturas

```{r}
#| echo: false
#| message: false

candidatura_by_day <- actions |>
  filter(data.list.name == 'Aplicados') |>
  group_by(data.card.id) |>
  filter(date == min(date)) |>
  mutate(date = as.Date(date)) |>
  group_by(date) |>
  summarise(total_candidatura = n())


# intervalo total da busca de emprego
inicio <- min(candidatura_by_day$date)
fim <- max(candidatura_by_day$date)

# gera sequência de todas as datas entre o primeiro e o último dia
todas_as_datas <- tibble(date = seq.Date(inicio, fim, by = "day"))

# une com os dias em que houve candidatura
dias_com_ou_sem <- todas_as_datas |>
  left_join(candidatura_by_day, by = "date") |>
  mutate(
    aplicou = !is.na(total_candidatura)
  )

# quantos dias sem candidatura
dias_sem <- dias_com_ou_sem |>
  summarise(
    total_dias = n(),
    dias_sem_aplicar = sum(!aplicou),
    pct_sem_aplicar = dias_sem_aplicar / total_dias * 100
  )

# Cálculos para o texto
total_dias <- nrow(candidatura_by_day)
total_candidaturas_periodo <- sum(candidatura_by_day$total_candidatura)
media_por_dia <- round(mean(candidatura_by_day$total_candidatura), 1)

# Estatísticas de julho
julho_data <- candidatura_by_day |>
  filter(date >= "2025-07-01" & date <= "2025-07-31")
candidaturas_julho <- sum(julho_data$total_candidatura)
dias_com_candidatura_julho <- nrow(julho_data)
media_julho <- round(mean(julho_data$total_candidatura), 1)
pico_maximo <- max(candidatura_by_day$total_candidatura)
```

```{r}
#| echo: false
#| message: false

# Gráfico de candidaturas por dia
plot_ly(
  candidatura_by_day,
  x = ~date, y = ~total_candidatura, type = "bar",
  marker = list(color = "#3B82F6", opacity = 0.8)
) |>
  layout(
    shapes = list(
      list(
        type = "rect",
        x0 = "2025-07-01", x1 = "2025-07-31",
        y0 = 0, y1 = max(candidatura_by_day$total_candidatura),
        fillcolor = "rgba(245,158,11,0.2)", line = list(width = 0)
      )
    ),
    title = list(
      text = "Evolução das Candidaturas Diárias (Maio–Agosto 2025)",
      font = list(size = 18, color = "#111827")
    ),
    xaxis = list(title = "Data"),
    yaxis = list(title = "Candidaturas por Dia"),
    plot_bgcolor = "#FFFFFF",
    paper_bgcolor = "#FFFFFF",
    showlegend = FALSE
  )


```

Ao longo de `r total_dias` dias ativos de busca, enviei `r total_candidaturas_periodo` candidaturas, com uma média de `r media_por_dia` aplicações por dia. Porém, os números contam uma história muito mais rica do que simples estatísticas.

### As Três Fases da Busca

1.  Início Empolgado (Maio-Junho)

```         
- Adaptação ao processo de candidaturas

- Curva de aprendizado nas aplicações

- Volume moderado enquanto ajustava o ritmo
```

2.  Foco em Entrevistas (Junho)

```         
- Queda natural no volume de novas aplicações

- Energia direcionada para processos em andamento

- Estratégia mais seletiva nas novas candidaturas
```

3.  Intensificação Estratégica (Julho)

```         
- `r candidaturas_julho` candidaturas concentradas em `r dias_com_candidatura_julho` dias

- Média de `r media_julho` aplicações/dia, quase o dobro do período geral

- Pico de `r pico_maximo` candidaturas em um único dia
```

### O Insight por Trás dos Números

Julho representou um ponto de virada consciente: após avaliar meus resultados iniciais, decidi aumentar significativamente o volume e aplicar para tudo que se encaixava genuinamente no meu perfil. Foi uma estratégia intencional para ampliar o leque de oportunidades e compensar a taxa de conversão naturalmente baixa do mercado.

Essa fase intensa não foi sobre quantidade pela quantidade, mas sobre otimizar o funil de candidaturas com base nos aprendizados dos primeiros dois meses.

### O Resultado Dessa Estratégia

A intensificação de julho foi crucial para:

-   Gerar um pipeline robusto de oportunidades

-   Aumentar a diversidade de empresas e tipos de vaga

-   Criar momentum psicológico durante a busca

-   Resultar na contratação final em agosto

## Ritmo e Consistência na Busca por Oportunidades

```{r}
#| echo: false
#| message: false
# Cálculos para o texto
media_aplicacoes <- candidatura_by_day |> 
  summarise(avg_apply = mean(total_candidatura)) |> 
  pull(avg_apply) |> round(1)

max_aplicacoes <- candidatura_by_day |> 
  summarise(max_apply = max(total_candidatura)) |> 
  pull(max_apply)

min_aplicacoes <- candidatura_by_day |> 
  summarise(min_apply = min(total_candidatura)) |> 
  pull(min_apply)

total_dias_periodo <- dias_sem$total_dias
dias_sem_aplicar <- dias_sem$dias_sem_aplicar
pct_sem_aplicar <- dias_sem$pct_sem_aplicar |> round(1)
dias_com_aplicar <- total_dias_periodo - dias_sem_aplicar
```

### O Panorama do Período Ativo

`r total_dias_periodo` dias de busca ativa (de `r format(inicio, "%d/%m")` a `r format(fim, "%d/%m/%Y")`)

`r dias_com_aplicar` dias com candidaturas vs `r dias_sem_aplicar` dias sem aplicações

`r pct_sem_aplicar`% do tempo dedicado a outras atividades da busca

### A Dinâmica dos Dias de Aplicação

```{r}
#| echo: false
#| message: false
# Estatísticas resumidas
candidatura_by_day |>
  summarise(
    avg_apply = mean(total_candidatura),
    max_apply = max(total_candidatura),
    min_apply = min(total_candidatura)
  ) |> 
  knitr::kable(
    col.names = c("Média por Dia", "Máximo em um Dia", "Mínimo em um Dia"),
    align = "c"
  )

```

Nos dias em que efetivamente enviei candidaturas, o volume foi significativo:

-   Média de `r media_aplicacoes` vagas/dia nos dias ativos

-   Pico de `r max_aplicacoes` candidaturas em um único dia

-   Mínimo de `r min_aplicacoes` vaga nos dias mais seletivos

### O Verdadeiro Significado dos "Dias Vazios"

Os `r dias_sem_aplicar` dias sem novas aplicações (40% do tempo) não representam falta de produtividade, mas sim a natureza multifacetada de uma busca estratégica por emprego."

O que realmente acontecia nesses dias:

✅ Entrevistas e processos técnicos - etapas avançadas consumindo tempo

✅ Avaliação de ofertas - análise detalhada de propostas recebidas

✅ Gestão do pipeline - acompanhamento de candidaturas pendentes

✅ Busca qualificada - pesquisa por vagas alinhadas ao invés de aplicações em massa

✅ Descanso estratégico - evitar burnout em um processo mentalmente desgastante

\*\* Padrão de "Compensação Inteligente"\*\*

A análise revela um padrão eficiente: dias focados em aplicações intensivas (`r media_aplicacoes` vagas/dia) compensavam os períodos dedicados a outras atividades essenciais do processo.

Isso demonstra uma abordagem madura onde quantidade e qualidade se equilibraram conforme as necessidades de cada fase:

-   Dias de volume: Ampliação do funil de oportunidades

-   Dias de qualidade: Foco em processos avançados e vagas premium

**Lição para Quem Busca Recolocação**

Buscar emprego é um trabalho em tempo integral que vai muito além de enviar currículos. A métrica importante não é 'dias sem aplicar', mas sim progresso geral no processo, que inclui entrevistas, networking, estudos e autocuidado.

### Habilidades Mais Demandadas pelo Mercado

```{r}
#| echo: false
#| message: false
  df_in <- requisitos
# 1) filtra e conta
df_clean <- df_in |> filter(habilidade != "analise de dados")

counts <- df_clean |>
  mutate(tipo = str_to_lower(tipo_habilidade)) |>
  count(tipo, habilidade, name = "n")

top_n <- 10
top_tecnica <- counts |>
  filter(tipo == "tecnica") |>
  slice_max(n, n = top_n, with_ties = FALSE) |>
  arrange(desc(n))

top_comport <- counts |>
  filter(tipo == "comportamental") |>
  slice_max(n, n = top_n, with_ties = FALSE) |>
  arrange(desc(n))

# 2) prepara fatores (ordem visual descendente)
top_tecnica <- top_tecnica |> mutate(y = factor(habilidade, levels = rev(habilidade)))
top_comport <- top_comport |> mutate(y = factor(habilidade, levels = rev(habilidade)))


# Cálculos para o texto - atualizados com seus dados
total_requisitos <- nrow(requisitos)
total_vagas_analisadas <- n_distinct(requisitos$id_vaga)

top_3_tecnicas <- top_tecnica |> slice_head(n = 3) |> pull(habilidade)
top_3_comportamentais <- top_comport |> slice_head(n = 3) |> pull(habilidade)

tecnica_mais_frequente <- top_tecnica |> slice_max(n, n = 1) |> pull(habilidade)
count_tecnica_top <- top_tecnica |> slice_max(n, n = 1) |> pull(n)

comport_mais_frequente <- top_comport |> slice_max(n, n = 1) |> pull(habilidade)
count_comport_top <- top_comport |> slice_max(n, n = 1) |> pull(n)

# Estatísticas adicionais
perc_sql <- round((302 / total_vagas_analisadas) * 100, 1)
perc_python <- round((235 / total_vagas_analisadas) * 100, 1)
perc_comunicacao <- round((326 / total_vagas_analisadas) * 100, 1)
```

Analisei `r total_requisitos` requisitos extraídos de `r total_vagas_analisadas` vagas para identificar quais habilidades eram mais valorizadas pelo mercado. Separei as competências em técnicas e comportamentais para entender o perfil profissional mais demandado.

### Top Habilidades Técnicas

```{r}
#| echo: false
#| message: false



# 3) cria os dois traces (esquerda: técnica, direita: comportamental)
#    - no painel esquerdo vamos plotar valores positivos mas inverter o xaxis para parecer espelhado
trace_left <- plot_ly(
  top_tecnica,
  x = ~n,
  y = ~y,
  type = "bar",
  orientation = "h",
  name = "Técnica",
  marker = list(color = "#1f77b4"),
  text = ~n,
  textposition = "inside",
  hovertemplate = paste("<b>%{y}</b><br>Técnica: %{x}<extra></extra>")
)

trace_right <- plot_ly(
  top_comport,
  x = ~n,
  y = ~y,
  type = "bar",
  orientation = "h",
  name = "Comportamental",
  marker = list(color = "#ff7f0e"),
  text = ~n,
  textposition = "inside",
  hovertemplate = paste("<b>%{y}</b><br>Comportamental: %{x}<extra></extra>")
)

# 4) monta os subplots lado a lado
# shareY = FALSE porque os y são categorias diferentes; widths controla proporção
fig <- subplot(trace_left, trace_right, nrows = 1, shareY = FALSE, widths = c(0.5, 0.5), titleX = TRUE, titleY = TRUE)

# 5) ajuste das margens / eixos para que o painel direito mostre y labels à direita
fig <- fig |>
  layout(
    title = "Técnicas (esquerda)  |  Comportamentais (direita) — Top 10",
    showlegend = TRUE,
    legend = list(orientation = "h", x = 0.25, y = 1.05),
    margin = list(l = 80, r = 80, t = 80, b = 40)
  ) |>
  # painel esquerdo: xaxis (1) normal, yaxis (1) labels à esquerda (default)
  layout(xaxis = list(title = "# vagas"),
         yaxis = list(title = "", automargin = TRUE)) |>
  # painel direito: xaxis2 e yaxis2 correspondem ao 2º subplot
  layout(xaxis2 = list(title = "# vagas"),
         yaxis2 = list(title = "", automargin = TRUE, side = "right"))

# 6) opcional: inverter a direção do painel esquerdo (para efeito espelhado)
#    se preferir que o lado esquerdo pareça espelhado ao direito, podemos inverter seu xaxis com autorange = "reversed"
fig <- fig |> layout(xaxis = list(autorange = "reversed"))

fig
```

As habilidades técnicas revelam uma base clara de conhecimentos exigidos:

-   `r tecnica_mais_frequente` é absoluta: `r count_tecnica_top` menções (`r perc_sql`% das vagas)

-   `r top_3_tecnicas[2]` segue como segundo lugar com `r top_tecnica$n[2]` citações (`r perc_python`%)

-   `r top_3_tecnicas[3]` completa o pódio com `r top_tecnica$n[3]` ocorrências

#### Hierarquia clara de ferramentas:

1.  Banco de dados: SQL (fundamental)

2.  Programação: Python (dominante sobre R)

3.  Visualização: Power BI (lidera frente ao Tableau)

### Soft Skills: Comunicação como Imperativo

No aspecto comportamental, os dados são ainda mais reveladores:

-   `r comport_mais_frequente` é a soft skill mais citada de toda a análise: `r count_comport_top` menções (`r perc_comunicacao`% das vagas)

-   `r top_3_comportamentais[2]` com `r top_comport$n[2]` ocorrências

-   `r top_3_comportamentais[3]` aparece `r top_comport$n[3]` vezes

Destaque crucial: "Boa comunicação" aparece mais que qualquer habilidade técnica, incluindo SQL. Isso reforça que o analista de dados moderno precisa ser, antes de tudo, um comunicador eficaz.

### O Profissional Completo de Dados em 2025

A análise pinta um retrato claro do profissional desejado:

💻 Base Técnica Não-Negociável

-   SQL (`r perc_sql`% das vagas) - fundamental para acesso e manipulação de dados

-   Python (`r perc_python`%) - para análise estatística e machine learning

-   Ferramentas de BI - Power BI dominante, mas Tableau ainda relevante

🎯 Competências Comportamentais Essenciais

-   Comunicação (`r perc_comunicacao`%) - traduzir dados em insights acionáveis

-   Trabalho em equipe - colaboração em ambientes multidisciplinares

-   Resolução de problemas - abordagem analítica para desafios complexos

### Visualização por Wordcloud

```{r}
#| echo: false
#| message: false
box::use(tibble[tibble],
         tidytext[unnest_tokens],
         stringr[str_replace_all, str_trim, str_to_lower],
         stopwords[stopwords],
         dplyr[slice_head, if_else],
         wordcloud2[wordcloud2]
         )

bow_wordcloud2 <- function(docs, max_words = 200, language = "pt") {
  df <- tibble(text = docs) |> mutate(doc_id = row_number())
  
  tokens <- df |>
    unnest_tokens(word, text) |>
    mutate(word = str_replace_all(word, "[^\\p{L}\\p{N}]+", " ")) |>
    mutate(word = str_trim(word)) |>
    filter(word != "") |>
    mutate(word = str_to_lower(word))
  
  stop_pt <- stopwords::stopwords("pt")
  # Lista personalizada de palavras para manter - incluindo "comunicação"
  palavras_manter <- c("comunicação", "comunicacao", "trabalho", "equipe")
  
  freq <- tokens |>
    filter(!(word %in% stop_pt) | word %in% palavras_manter) |>  # Mantém palavras importantes
    filter(!word %in% c('boa', 'ser', 'estar')) |>  # Remove outras palavras irrelevantes
    count(word, sort = TRUE) |>
    slice_head(n = max_words)
  
  # rename 'n' para 'freq' e garantir data.frame / numeric
  if ("n" %in% names(freq)) names(freq)[names(freq) == "n"] <- "freq"
  freq <- as.data.frame(freq)
  freq$freq <- as.numeric(freq$freq)

  # conferir o top 5 antes de plotar (debug)
  print(head(freq, 10))
  # wordcloud2 retorna um widget HTML (interativo)
  wordcloud2(freq, size = 1)
}

requisitos_comportamento <-  requisitos |>
  filter(tipo_habilidade == 'comportamental')

requisitos_tecnica <- requisitos |>
  filter(tipo_habilidade == 'tecnica')




```

```{r}
#| echo: false
#| message: false
# Wordcloud habilidades técnicas
bow_wordcloud2(requisitos_tecnica$habilidade, max_words = 100)
```

```{r}
#| echo: false
#| message: false
# Wordcloud habilidades comportamentais
bow_wordcloud2(requisitos_comportamento$habilidade, max_words = 100)
```

### Insight Estratégico para Desenvolvimento

Estes dados orientam prioridades de aprendizado:

1.  SQL é obrigatório - aparece em quase todas as vagas técnicas

2.  Python + Power BI formam o stack básico moderno

3.  Comunicação não é opcional - é a soft skill mais valorizada

4.  R ainda tem espaço (`r top_tecnica$n[8]` menções), mas Python domina

"O mercado busca técnicos que comunicam bem, não apenas gênios do código. A habilidade de explicar insights complexos em termos simples é tão valiosa quanto a capacidade de gerá-los."
